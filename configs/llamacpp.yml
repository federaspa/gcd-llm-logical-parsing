# Configuration file for llamacpp
n_gpu_layers: 76  # Use all available GPU layers
n_batch: 512
n_threads: 12      # Number of CPU threads
n_ctx: 4096       # Context window size
verbose: false