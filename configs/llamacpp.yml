# Configuration file for llamacpp
n_gpu_layers: -1  # Use all available GPU layers
n_batch: 512
n_threads: 1      # Number of CPU threads
# n_ctx: 10300       # Context window size
# max_new_tokens: 2048  # Maximum tokens to generate
verbose: true