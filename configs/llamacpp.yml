# Configuration file for llamacpp
n_gpu_layers: -1  # Use all available GPU layers
n_ctx: 5120       # Context window size
max_tokens: 1536  # Maximum tokens to generate
n_threads: 1      # Number of CPU threads
n_batch: 512
verbose: false