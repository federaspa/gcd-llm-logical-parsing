# Configuration file for llamacpp
n_gpu_layers: -1  # Use all available GPU layers
n_batch: 512
n_threads: 1      # Number of CPU threads
n_ctx: 4096       # Context window size
verbose: false