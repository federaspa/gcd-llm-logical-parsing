{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check balancing of FOLIO dataset\n",
    "import json\n",
    "with open('data/FOLIO/train.json', 'r') as f:\n",
    "    folio = json.load(f)\n",
    "    \n",
    "with open('data/FOLIO/dev.json', 'r') as f:\n",
    "    f_dev = json.load(f)\n",
    "    \n",
    "    \n",
    "with open('data/LogicNLI/train.json', 'r') as f:\n",
    "    lni = json.load(f)\n",
    "\n",
    "with open('data/LogicNLI/dev.json', 'r') as f:\n",
    "    l_dev = json.load(f)\n",
    "\n",
    "answers_folio = [sample['answer'] for sample in folio]\n",
    "answers_folio_dev = [sample['answer'] for sample in f_dev]\n",
    "\n",
    "answers_lni = [sample['answer'] for sample in lni]\n",
    "answers_lni_dev = [sample['answer'] for sample in l_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "answers_folio = pd.Series(answers_folio)\n",
    "answers_folio_dev = pd.Series(answers_folio_dev)\n",
    "\n",
    "answers_lni = pd.Series(answers_lni)\n",
    "answers_lni_dev = pd.Series(answers_lni_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_folio_all = pd.concat([answers_folio, answers_folio_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_folio_all.value_counts()/len(answers_folio_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_folio.value_counts()/len(answers_folio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_folio_dev.value_counts()/len(answers_folio_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_lni.value_counts()/len(answers_lni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Get batch response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "def parse_batch_preds(path):\n",
    "    \n",
    "    with open('data/LogicNLI/dev.json', 'r') as f:\n",
    "        dev_data = json.load(f)\n",
    "        \n",
    "    dev_data_dict = {d['id']: d for d in dev_data}\n",
    "    \n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    raw_results = [json.loads(line) for line in lines]\n",
    "    parsed_results = {}\n",
    "\n",
    "    for result in raw_results:\n",
    "        result_id = result[\"custom_id\"]\n",
    "        response = result['response']['body']['choices'][0]['message']['content'].replace('`', '').replace('json\\n', '').replace('\"\"\"', '')\n",
    "        \n",
    "        response_json_str = re.sub(r'\\\\{',  '{', response)\n",
    "        response_json_str = re.sub(r'\\\\}',  '}', response_json_str)\n",
    "        \n",
    "        response_json = json.loads(response_json_str)\n",
    "\n",
    "\n",
    "        result_context = dev_data_dict[int(result_id)]['context']\n",
    "        result_question = dev_data_dict[int(result_id)]['question']\n",
    "\n",
    "    \n",
    "        parsed_results[int(result_id)] = {\n",
    "                \"context\": result_context,\n",
    "                \"question\": result_question,\n",
    "                \"logic_predicates\": response_json['First-Order-Logic Predicates'].split('\\n')\n",
    "            }\n",
    "    \n",
    "\n",
    "        \n",
    "    return parsed_results\n",
    "\n",
    "def parse_batch_progs(progs_path, preds_path):\n",
    "    \n",
    "    with open('data/LogicNLI/dev.json', 'r') as f:\n",
    "        dev_data = json.load(f)\n",
    "        \n",
    "    dev_data_dict = {d['id']: d for d in dev_data}\n",
    "    \n",
    "    with open(preds_path, \"r\") as f:\n",
    "        preds = json.load(f)\n",
    "        \n",
    "    preds_dict = {int(k): v for k, v in preds.items()} \n",
    "        \n",
    "    \n",
    "    with open(progs_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    raw_results = [json.loads(line) for line in lines]\n",
    "    parsed_results = []\n",
    "\n",
    "    for result in raw_results:\n",
    "\n",
    "        result_id = result[\"custom_id\"]\n",
    "\n",
    "\n",
    "        if result['response']['body']['choices'][0]['finish_reason'] != 'stop':\n",
    "            parsed_results.append({\n",
    "                    \"id\": int(result_id),\n",
    "                    \"context\": dev_data_dict[int(result_id)]['context'],\n",
    "                    \"question\": dev_data_dict[int(result_id)]['question'],\n",
    "                    \"answer\": dev_data_dict[int(result_id)]['answer'],\n",
    "                    \"raw_logic_programs\": json.loads('{\\n\"First-Order-Logic Rules\": \"No rules found\",\\n\"First-Order-Logic Question\": \"No question found\"\\n}'),\n",
    "                    \"predicates\": preds_dict[int(result_id)]['logic_predicates']\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "\n",
    "        response = json.loads(result['response']['body']['choices'][0]['message']['content'])\n",
    "        \n",
    "        parsed_results.append({\n",
    "                \"id\": int(result_id),\n",
    "                \"context\": dev_data_dict[int(result_id)]['context'],\n",
    "                \"question\": dev_data_dict[int(result_id)]['question'],\n",
    "                \"answer\": dev_data_dict[int(result_id)]['answer'],\n",
    "                \"raw_logic_programs\": response,\n",
    "                \"predicates\": preds_dict[int(result_id)]['logic_predicates']\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    return parsed_results\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = client.batches.retrieve(\"batch_xNhzhGB0yOgxUHzUUKNZEEv8\")\n",
    "\n",
    "if batch.status == \"completed\":\n",
    "    print(\"Batch is completed\")\n",
    "    out_file_id = batch.output_file_id\n",
    "    content = client.files.content(out_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.write_to_file(\".tmp/LogicNLI_dev_gpt-4o_dynamic.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = parse_batch_progs('.tmp/LogicNLI_dev_gpt-4o_dynamic.jsonl', 'outputs_3/logic_predicates/LogicNLI_dev_gpt-4o.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs_3/logic_programs/LogicNLI_dev_gpt-4o_dynamic.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "refiner_path = 'here/is/a/path/to/a/file.txt'\n",
    "refiner_path.split('/')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def compare_evaluate(result_path, dataset_name, split, model_name, prompt_mode, backup, self_refine_round, output_path):\n",
    "    result_file = os.path.join(result_path, f'self-refine-{self_refine_round}_{dataset_name}_{split}_{model_name}_{prompt_mode}_backup-{backup}.json')\n",
    "    original_result_file = os.path.join(result_path, f'{dataset_name}_{split}_{model_name}_{prompt_mode}_backup-{backup}.json')\n",
    "\n",
    "\n",
    "    with open(original_result_file, 'r') as f:\n",
    "        original_results = json.load(f)\n",
    "\n",
    "    with open(result_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "        \n",
    "        \n",
    "    original_dict = {sample['id']: sample for sample in original_results}\n",
    "    results_dict = {sample['id']: sample for sample in results}\n",
    "    new_executable = [sample for sample_id, sample in results_dict.items() if sample['flag']=='success' and original_dict[sample_id]['flag'] != 'success']\n",
    "\n",
    "    if not os.path.exists(f'compared_evaluation/{output_path}'):\n",
    "        os.makedirs(f'compared_evaluation/{output_path}')\n",
    "\n",
    "    with open(f'compared_evaluation/{output_path}/self-refine-{self_refine_round}_{dataset_name}_{split}_{model_name}_{prompt_mode}_backup-{backup}.json', 'w') as f:\n",
    "        json.dump(new_executable, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = 'outputs_llama3_70B/logic_inference'\n",
    "dataset_name = 'FOLIO'\n",
    "split = 'dev'\n",
    "model_names = ['gpt-3.5-turbo', 'gpt-4-turbo', 'gpt-4o']\n",
    "prompt_mode = 'dynamic'\n",
    "backup = 'random'\n",
    "output_paths = ['llama3_70B', 'mixtral_8x7B', 'mistral_7B']\n",
    "\n",
    "for output_path in output_paths:\n",
    "    for model_name in model_names:\n",
    "        for self_refine_round in range(1, 4):\n",
    "            compare_evaluate(result_path, dataset_name, split, model_name, prompt_mode, backup, self_refine_round, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.split(result_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding('cl100k_base')\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/LogicNLI/train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    total = 0\n",
    "    for d in data:\n",
    "\n",
    "        total += num_tokens_from_string(' '.join(d['context']))\n",
    "        total += num_tokens_from_string(' '.join(d['context_fol']))\n",
    "        total += num_tokens_from_string(d['question'])\n",
    "        total += num_tokens_from_string(d['question_fol'])\n",
    "    \n",
    "    \n",
    "with open('data/LogicNLI/dev.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    for d in data:\n",
    "\n",
    "        total += num_tokens_from_string(' '.join(d['context']))\n",
    "        total += num_tokens_from_string(' '.join(d['context_fol']))\n",
    "        total += num_tokens_from_string(d['question'])\n",
    "        total += num_tokens_from_string(d['question_fol'])\n",
    "    \n",
    "    print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(total/1000000)*0.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert LogicNLI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/LogicNLI_original/dev_language.json', 'r') as f:\n",
    "    sample_language = json.load(f)\n",
    "\n",
    "with open('data/LogicNLI_original/dev_logic.json', 'r') as f:\n",
    "    sample_logic = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'entailment': 'A',\n",
    "    'contradiction': 'B',\n",
    "    'self_contradiction': 'B',\n",
    "    'neutral': 'C'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = []\n",
    "question_id = 0\n",
    "\n",
    "for story_id, ((data_id, data), (statement_id, statement)) in enumerate(zip(sample_logic.items(), sample_language.items())):\n",
    "    \n",
    "    context_fol = []\n",
    "    context = []\n",
    "    for (fact_id, fact), (fact_nl) in zip(data['facts'].items(), statement['facts']):\n",
    "        \n",
    "        subject = fact[0].lower()\n",
    "        attribute = fact[1].capitalize()\n",
    "        polarity = fact[2]\n",
    "        \n",
    "        context_fol.append(f\"{attribute}({subject})\" if polarity == '+' else f\"Â¬{attribute}({subject})\")\n",
    "        context.append(fact_nl)\n",
    "        \n",
    "    for (rule_id, rule_data), (rule_nl) in zip(data['rules'].items(), statement['rules']):\n",
    "        premise = []\n",
    "        \n",
    "        same_quant_subject = all(fact[0] == rule_data['p']['fact'][0][0] for fact in rule_data['p']['fact']) and all(fact[0] in ['all', 'exist'] for fact in rule_data['p']['fact'])\n",
    "        \n",
    "        for fact in rule_data['p']['fact']:\n",
    "            \n",
    "            attribute = fact[1].capitalize()\n",
    "            polarity = fact[2]\n",
    "            \n",
    "            if same_quant_subject:\n",
    "                quant = \"âx \" if fact[0] == 'exist' else \"âx \"\n",
    "                \n",
    "                premise.append(f'{attribute}(x)' if polarity == '+' else f'Â¬{attribute}(x)')\n",
    "            \n",
    "            else:\n",
    "                if fact[0] == 'exist':\n",
    "                    premise.append(f\"âx \" + (f'{attribute}(x)' if polarity == '+' else f'Â¬{attribute}(x)'))\n",
    "                elif fact[0] == 'all':\n",
    "                    premise.append(f\"âx \" + (f'{attribute}(x)' if polarity == '+' else f'Â¬{attribute}(x)'))\n",
    "                else:\n",
    "                    subject = fact[0].lower()\n",
    "                    premise.append(f'{attribute}({subject})' if polarity == '+' else f'Â¬{attribute}({subject})')\n",
    "                \n",
    "        if same_quant_subject:\n",
    "            premise_str = quant + '('\n",
    "        else:\n",
    "            premise_str = '('\n",
    "        premise_str += ' â¨ '.join(premise) if rule_data['p']['conj'] == 'or' else ' â§ '.join(premise)\n",
    "        premise_str += ')'\n",
    "\n",
    "        same_quant_subject = all(fact[0] == rule_data['q']['fact'][0][0] for fact in rule_data['q']['fact']) and all(fact[0] in ['all', 'exist'] for fact in rule_data['q']['fact'])\n",
    "\n",
    "        conclusion = []\n",
    "        for fact in rule_data['q']['fact']:\n",
    "            \n",
    "            attribute = fact[1].capitalize()\n",
    "            polarity = fact[2]\n",
    "            \n",
    "            if same_quant_subject:\n",
    "                quant = \"âx \" if fact[0] == 'exist' else \"âx \"\n",
    "                \n",
    "                conclusion.append(f'{attribute}(x)' if polarity == '+' else f'Â¬{attribute}(x)')\n",
    "            \n",
    "            else:\n",
    "                if fact[0] == 'exist':\n",
    "                    conclusion.append(f\"âx \" + (f'{attribute}(x)' if polarity == '+' else f'Â¬{attribute}(x)'))\n",
    "                elif fact[0] == 'all':\n",
    "                    conclusion.append(f\"âx \" + (f'{attribute}(x)' if polarity == '+' else f'Â¬{attribute}(x)'))\n",
    "                else:\n",
    "                    subject = fact[0].lower()\n",
    "                    conclusion.append(f'{attribute}({subject})' if polarity == '+' else f'Â¬{attribute}({subject})')\n",
    "                \n",
    "                \n",
    "        if same_quant_subject:\n",
    "            conclusion_str = quant + '('\n",
    "        else:\n",
    "            conclusion_str = '('\n",
    "        conclusion_str += ' â¨ '.join(conclusion) if rule_data['q']['conj'] == 'or' else ' â§ '.join(conclusion)\n",
    "        conclusion_str += ')'\n",
    "        \n",
    "        # print(conclusion_str)\n",
    "\n",
    "        if rule_data['type'] == 'imp':\n",
    "            context_fol.append(f\"({premise_str}) â ({conclusion_str})\")\n",
    "        else:\n",
    "            context_fol.append(f\"({premise_str}) â ({conclusion_str})\")\n",
    "            \n",
    "        context.append(rule_nl)\n",
    "    \n",
    "    \n",
    "    for ((_, question), (question_nl), (label)) in zip(data['statements'].items(), statement['statements'], statement['labels']):\n",
    "\n",
    "        if label == 'self_contradiction':\n",
    "            continue\n",
    "\n",
    "        attribute = question[1].capitalize()\n",
    "        polarity = question[2]\n",
    "        subject = question[0].lower()\n",
    "        \n",
    "        question_fol = f'{attribute}({subject})' if polarity == '+' else f'Â¬{attribute}({subject})'        \n",
    "        \n",
    "        output.append({\n",
    "            'id': int(question_id),\n",
    "            'story_id': int(story_id),\n",
    "            'context': context,\n",
    "            'context_fol': context_fol,\n",
    "            'question': question_nl,\n",
    "            'question_fol': question_fol,\n",
    "            'answer': label_mapping[label]\n",
    "        })\n",
    "        \n",
    "        question_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/LogicNLI/dev.json', 'w') as f:\n",
    "    json.dump(output, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/LogicNLI/train.json', 'r') as f:\n",
    "    a = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "reg = re.compile(r'(?!x|y|z|w)\\b\\w+\\s*\\([^\\)]+\\)')\n",
    "\n",
    "with open('data/LogicNLI/dev.json', 'r') as f:\n",
    "    train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_map = {\n",
    "    1: 'x',\n",
    "    2: 'x, y',\n",
    "    3: 'x, y, z',\n",
    "    4: 'x, y, z, w'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_predicate(predicate):\n",
    "    name = predicate.split('(')[0].strip()\n",
    "    arguments = [p.split() for p in predicate.split('(')[1][:-1].split(',')]\n",
    "    \n",
    "    n_arguments = len(arguments)\n",
    "    \n",
    "    clean_predicate = name + '(' + args_map[n_arguments] + ')'\n",
    "\n",
    "    return clean_predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for point in train:\n",
    "    formulas = ' '.join(point['context_fol'])\n",
    "    preds = list(set(re.findall(reg, formulas)))\n",
    "    \n",
    "    clean_preds = list(set(clean_predicate(pred) for pred in preds))\n",
    "    \n",
    "    point['logic_predicates'] = clean_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/LogicNLI/dev.json', 'w') as f:\n",
    "    json.dump(train, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/logic_predicates/FOLIO_dev_gpt-3.5-turbo.json', 'r') as f:\n",
    "    preds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/logic_programs/FOLIO_dev_gpt-3.5-turbo_static.json', 'r') as f:\n",
    "    progs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prog in progs:\n",
    "    if str(prog['id']) in preds:\n",
    "        prog['predicates'] = preds[str(prog['id'])]['logic_predicates']\n",
    "    else:\n",
    "        prog['predicates'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"\"\"\n",
    "String:\n",
    "'''\n",
    "1. e4 e5\n",
    "2. Nf3 Nc6\n",
    "3. Bb5 a6\n",
    "4. Ba4 Nf6\n",
    "5. O-O Be7\n",
    "6. Re1 b5\n",
    "7. Bb3 d6\n",
    "8. c3 O-O\n",
    "9. h3 Na5\n",
    "10. Bc2 c5\n",
    "11. d4 Qc7\n",
    "12. Nbd2 Nc6\n",
    "13. Nf1 Re8\n",
    "14. Ng3 Bf8\n",
    "15. d5 Nb8\n",
    "16. Nh4 g6\n",
    "17. Qf3 Bg7\n",
    "18. Bg5 Nbd7\n",
    "19. Rad1 h6\n",
    "20. Bc1 Nf8\n",
    "'''\n",
    "Grammar:\n",
    "'''\n",
    "root    ::= \"1. \" move \" \" move \"\\n\" ([1-9] [0-9]? \". \" move \" \" move \"\\n\")+\n",
    "move    ::= (pawn | nonpawn | castle) [+#]?\n",
    "\n",
    "# piece type, optional file/rank, optional capture, dest file & rank\n",
    "nonpawn ::= [NBKQR] [a-h]? [1-8]? \"x\"? [a-h] [1-8]\n",
    "\n",
    "# optional file & capture, dest file & rank, optional promotion\n",
    "pawn    ::= ([a-h] \"x\")? [a-h] [1-8] (\"=\" [NBKQR])?\n",
    "\n",
    "castle  ::= \"O-O\" \"-O\"?\n",
    "'''\n",
    "------\n",
    "String:\n",
    "'''\n",
    "x = 42\n",
    "y = x + 3\n",
    "z = (y - 2) * 5\n",
    "result = z / (x + y)\n",
    "a = 100 - 50\n",
    "b = (a + result) * 2\n",
    "c = b / 4\n",
    "d = c - a + x\n",
    "e = (d * 2) + (x - 3) / 7\n",
    "'''\n",
    "Grammar:\n",
    "'''\n",
    "root  ::= (expr \"=\" ws term \"\\n\")+\n",
    "expr  ::= term ([-+*/] term)*\n",
    "term  ::= ident | num | \"(\" ws expr \")\" ws\n",
    "ident ::= [a-z] [a-z0-9_]* ws\n",
    "num   ::= [0-9]+ ws\n",
    "ws    ::= [ \\t\\n]*\n",
    "'''\n",
    "------\n",
    "String:\n",
    "'''\n",
    "\"Czech(miroslav) â§ ChoralConductor(miroslav) â§ Specialize(miroslav, renaissance) â§ Specialize(miroslav, baroque)\",\n",
    "\"âx (ChoralConductor(x) â Musician(x))\",\n",
    "\"âx (Musician(x) â Love(x, music))\",\n",
    "\"Book(methodOfStudyingGregorianChant) â§ Author(miroslav, methodOfStudyingGregorianChant) â§ Publish(methodOfStudyingGregorianChant, year1946)\"\n",
    "'''\n",
    "Grammar:\n",
    "'''\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description = \"\"\"\n",
    "Given a string, the task is to write the grammar that can generate that string, in EBNF format. To do so, follow the provided examples\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_grammar = \"\"\"\n",
    "root ::= ProductionList \n",
    "\n",
    "ProductionList ::= Production ProductionList | Production \n",
    "\n",
    "Production ::= NonTerminal \"::=\" RHS \n",
    "\n",
    "RHS ::= Symbol RHS | Symbol \n",
    "\n",
    "Symbol ::= Terminal | NonTerminal | \"|\" \n",
    "\n",
    "NonTerminal ::= [A-Z]\n",
    "\n",
    "Terminal ::= [a-z]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp.llama import LlamaGrammar\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model_path = \"GCD/llms/mistral-7b-instruct-v0.2.Q6_K.gguf\"\n",
    "n_ctx = 2048\n",
    "n_gpu_layers = -1\n",
    "n_batch = 512\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx = n_ctx,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose = False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# max_tokens=50 \n",
    "# temperature=0.01\n",
    "# frequency_penalty=0.0\n",
    "# repeat_penalty=1.1\n",
    "# # presence_penalty=0.0\n",
    "# top_p=0.9\n",
    "# top_k=20\n",
    "stop=['------']\n",
    "\n",
    "grammar = LlamaGrammar.from_string(raw_grammar, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = llm.create_chat_completion(\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": task_description},\n",
    "    {\"role\": \"user\", \"content\": user}\n",
    "    ],\n",
    "max_tokens = 100,\n",
    "\n",
    "# frequency_penalty = frequency_penalty,\n",
    "# repeat_penalty = repeat_penalty,\n",
    "# # presence_penalty = presence_penalty,\n",
    "\n",
    "# temperature=temperature,\n",
    "\n",
    "# top_p=top_p,\n",
    "# top_k=top_k,\n",
    "\n",
    "# stop = stop,\n",
    "grammar = grammar,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from nltk.inference.prover9 import Prover9Command\n",
    "from nltk.sem.logic import *\n",
    "\n",
    "from models.symbolic_solvers.fol_solver.Formula_util import FOL_Formula\n",
    "from models.symbolic_solvers.fol_solver.fol_prover9_parser import Prover9_FOL_Formula\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "os.environ['PROVER9'] = './models/symbolic_solvers/Prover9/bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples removed from dev:\n",
    "\n",
    "- 66\n",
    "- 67\n",
    "- 68\n",
    "- 138\n",
    "- 139\n",
    "- 171\n",
    "\n",
    "Samples removed from train:\n",
    "\n",
    "- 691"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = Expression.fromstring('all x.(Roundel(x) -> -Higher(x) -> Lower(x))')\n",
    "new = Expression.fromstring('all x y.exists z(Know(x, z) & Know(y, z) & UniversalLanguage(z)) -> Communicate(x, y)')\n",
    "\n",
    "original.equiv(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(all x.(GSCjasdhjvfd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âx (WantlongVacation(x) â (Love(x, summer) â§ Â¬Love(x, spring) â§ Â¬Love(x, fall) â§ Â¬Love(x, winter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_fol= ['âx (nail(x) â metal(x))',\n",
    " 'âx (metal(x) â conductive(x))']\n",
    "\n",
    "question_fol = 'âx (nail(x) â conductive(x))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ass= []\n",
    "\n",
    "for s in context_fol:\n",
    "    fol_formula = FOL_Formula(s)\n",
    "\n",
    "    if fol_formula.is_valid:\n",
    "        prover9_formula = Prover9_FOL_Formula(fol_formula).formula\n",
    "        # \n",
    "        ass.append(Expression.fromstring(prover9_formula))\n",
    "    else:\n",
    "        print(f'error: ', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = Expression.fromstring(Prover9_FOL_Formula(FOL_Formula(question_fol)).formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prover = Prover9Command(goal, ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = NegatedExpression(goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prover = Prover9Command(goal, ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prover.prove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prover.proof(simplify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('old/outputs/logic_inference/FOLIO_dev_gpt-3.5-turbo_static_text_backup-LLM.json', 'r') as f:\n",
    "    old_inference = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/logic_inference/FOLIO_dev_gpt-3.5-turbo_static_text_backup-LLM.json', 'r') as f:\n",
    "    new_inference = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parsing/new_formulas_v0/FOLIO_dev_gpt-3.5-turbo_static_text.json', 'r') as j:\n",
    "    folio_v0_train_parsed = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/FOLIO/dev.json', 'r') as j:\n",
    "    folio_v0_train = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folio_v0_train_dict = {val['id']: val for val in folio_v0_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folio_v0_train_parsed_dict = {val['id']: val for val in folio_v0_train_parsed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in folio_v0_train_dict.keys():\n",
    "    folio_v0_train_dict[id]['context_fol'] = folio_v0_train_parsed_dict[id]['assumptions']\n",
    "    folio_v0_train_dict[id]['question_fol'] = folio_v0_train_parsed_dict[id]['goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_folio_v0_train = [val for key, val in folio_v0_train_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parsed_data/FOLIO/dev.json', 'w') as j:\n",
    "    json.dump(new_folio_v0_train, j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
